{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import tsahelper as tsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# Constants\n",
    "#\n",
    "# INPUT_FOLDER:                 The folder that contains the source data\n",
    "#\n",
    "# PREPROCESSED_DATA_FOLDER:     The folder that contains preprocessed .npy files \n",
    "# \n",
    "# STAGE1_LABELS:                The CSV file containing the labels by subject\n",
    "#\n",
    "# THREAT_ZONE:                  Threat Zone to train on (actual number not 0 based)\n",
    "#\n",
    "# BATCH_SIZE:                   Number of Subjects per batch\n",
    "#\n",
    "# EXAMPLES_PER_SUBJECT          Number of examples generated per subject\n",
    "#\n",
    "# FILE_LIST:                    A list of the preprocessed .npy files to batch\n",
    "# \n",
    "# TRAIN_TEST_SPLIT_RATIO:       Ratio to split the FILE_LIST between train and test\n",
    "#\n",
    "# TRAIN_SET_FILE_LIST:          The list of .npy files to be used for training\n",
    "#\n",
    "# TEST_SET_FILE_LIST:           The list of .npy files to be used for testing\n",
    "#\n",
    "# IMAGE_DIM:                    The height and width of the images in pixels\n",
    "#\n",
    "# LEARNING_RATE                 Learning rate for the neural network\n",
    "#\n",
    "# N_TRAIN_STEPS                 The number of train steps (epochs) to run\n",
    "#\n",
    "# TRAIN_PATH                    Place to store the tensorboard logs\n",
    "#\n",
    "# MODEL_PATH                    Path where model files are stored\n",
    "#\n",
    "# MODEL_NAME                    Name of the model files\n",
    "#\n",
    "#----------------------------------------------------------------------------------------\n",
    "INPUT_FOLDER = 'tsa_datasets/stage1/aps'\n",
    "PREPROCESSED_DATA_FOLDER = 'preprocessed/'\n",
    "STAGE1_LABELS = 'stage1_labels.csv'\n",
    "THREAT_ZONE = 2\n",
    "BATCH_SIZE = 16\n",
    "EXAMPLES_PER_SUBJECT = 182\n",
    "\n",
    "FILE_LIST = []\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.2\n",
    "TRAIN_SET_FILE_LIST = []\n",
    "TEST_SET_FILE_LIST = []\n",
    "\n",
    "IMAGE_DIM = 250\n",
    "LEARNING_RATE = 1e-3\n",
    "N_TRAIN_STEPS = 1\n",
    "TRAIN_PATH = 'tsa_logs/train/'\n",
    "MODEL_PATH = 'tsa_logs/model/'\n",
    "MODEL_NAME = ('tsa-{}-lr-{}-{}-{}-tz-{}'.format('alexnet-v0.1', LEARNING_RATE, IMAGE_DIM, \n",
    "                                                IMAGE_DIM, THREAT_ZONE )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# preprocess_tsa_data(): preprocesses the tsa datasets\n",
    "#\n",
    "# parameters:      none\n",
    "#\n",
    "# returns:         none\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_tsa_data():\n",
    "    \n",
    "    # OPTION 1: get a list of all subjects for which there are labels\n",
    "    #df = pd.read_csv(STAGE1_LABELS)\n",
    "    #df['Subject'], df['Zone'] = df['Id'].str.split('_',1).str\n",
    "    #SUBJECT_LIST = df['Subject'].unique()\n",
    "\n",
    "    # OPTION 2: get a list of all subjects for whom there is data\n",
    "    #SUBJECT_LIST = [os.path.splitext(subject)[0] for subject in os.listdir(INPUT_FOLDER)]\n",
    "    \n",
    "    # OPTION 3: get a list of subjects for small bore test purposes\n",
    "    SUBJECT_LIST = ['00360f79fd6e02781457eda48f85da90','0043db5e8c819bffc15261b1f1ac5e42',\n",
    "                    '0050492f92e22eed3474ae3a6fc907fa','006ec59fa59dd80a64c85347eef810c7',\n",
    "                    '0097503ee9fa0606559c56458b281a08','011516ab0eca7cad7f5257672ddde70e']\n",
    "    \n",
    "    # intialize tracking and saving items\n",
    "    batch_num = 1\n",
    "    threat_zone_examples = []\n",
    "    start_time = timer()\n",
    "    \n",
    "    for subject in SUBJECT_LIST:\n",
    "\n",
    "        # read in the images\n",
    "        print('--------------------------------------------------------------')\n",
    "        print('t+> {:5.3f} |Reading images for subject #: {}'.format(timer()-start_time, \n",
    "                                                                     subject))\n",
    "        print('--------------------------------------------------------------')\n",
    "        images = tsa.read_data('stage1_aps_' + subject + '.aps')\n",
    "\n",
    "        # transpose so that the slice is the first dimension shape(16, 620, 512)\n",
    "        images = images.transpose()\n",
    "\n",
    "        # for each threat zone, loop through each image, mask off the zone and then crop it\n",
    "        for tz_num, threat_zone_x_crop_dims in enumerate(zip(tsa.zone_slice_list, \n",
    "                                                             tsa.zone_crop_list)):\n",
    "\n",
    "            threat_zone = threat_zone_x_crop_dims[0]\n",
    "            crop_dims = threat_zone_x_crop_dims[1]\n",
    "\n",
    "            # get label\n",
    "            label = np.array(tsa.get_subject_zone_label(tz_num, \n",
    "                             tsa.get_subject_labels(STAGE1_LABELS, subject)))\n",
    "\n",
    "            for img_num, img in enumerate(images):\n",
    "\n",
    "                print('Threat Zone:Image -> {}:{}'.format(tz_num, img_num))\n",
    "                print('Threat Zone Label -> {}'.format(label))\n",
    "                \n",
    "                if threat_zone[img_num] is not None:\n",
    "\n",
    "                    # correct the orientation of the image\n",
    "                    print('-> reorienting base image') \n",
    "                    base_img = np.flipud(img)\n",
    "                    print('-> shape {}|mean={}'.format(base_img.shape, \n",
    "                                                       base_img.mean()))\n",
    "\n",
    "                    # convert to grayscale\n",
    "                    print('-> converting to grayscale')\n",
    "                    rescaled_img = tsa.convert_to_grayscale(base_img)\n",
    "                    print('-> shape {}|mean={}'.format(rescaled_img.shape, \n",
    "                                                       rescaled_img.mean()))\n",
    "\n",
    "                    # spread the spectrum to improve contrast\n",
    "                    print('-> spreading spectrum')\n",
    "                    high_contrast_img = tsa.spread_spectrum(rescaled_img)\n",
    "                    print('-> shape {}|mean={}'.format(high_contrast_img.shape,\n",
    "                                                       high_contrast_img.mean()))\n",
    "\n",
    "                    # get the masked image\n",
    "                    print('-> masking image')\n",
    "                    masked_img = tsa.roi(high_contrast_img, threat_zone[img_num])\n",
    "                    print('-> shape {}|mean={}'.format(masked_img.shape, \n",
    "                                                       masked_img.mean()))\n",
    "\n",
    "                    # crop the image\n",
    "                    print('-> cropping image')\n",
    "                    cropped_img = tsa.crop(masked_img, crop_dims[img_num])\n",
    "                    print('-> shape {}|mean={}'.format(cropped_img.shape, \n",
    "                                                       cropped_img.mean()))\n",
    "\n",
    "                    # normalize the image\n",
    "                    print('-> normalizing image')\n",
    "                    normalized_img = tsa.normalize(cropped_img)\n",
    "                    print('-> shape {}|mean={}'.format(normalized_img.shape, \n",
    "                                                       normalized_img.mean()))\n",
    "\n",
    "                    # zero center the image\n",
    "                    print('-> zero centering')\n",
    "                    zero_centered_img = tsa.zero_center(normalized_img)\n",
    "                    print('-> shape {}|mean={}'.format(zero_centered_img.shape, \n",
    "                                                       zero_centered_img.mean()))\n",
    "\n",
    "                    # append the features and labels to this threat zone's example array\n",
    "                    print ('-> appending example to threat zone {}'.format(tz_num))\n",
    "                    threat_zone_examples.append([[tz_num], zero_centered_img, label])\n",
    "#                     print(\"************************************************\")\n",
    "#                     print(threat_zone_examples[0][1][0])\n",
    "#                     print(\"************************************************\")\n",
    "                    print ('-> shape {:d}:{:d}:{:d}:{:d}:{:d}:{:d}'.format(\n",
    "                                                         len(threat_zone_examples),\n",
    "                                                         len(threat_zone_examples[0]),\n",
    "                                                         len(threat_zone_examples[0][0]),\n",
    "                                                         len(threat_zone_examples[0][1][0]),\n",
    "                                                         len(threat_zone_examples[0][1][1]),\n",
    "                                                         len(threat_zone_examples[0][2])))\n",
    "                else:\n",
    "                    print('-> No view of tz:{} in img:{}. Skipping to next...'.format( \n",
    "                                tz_num, img_num))\n",
    "                print('------------------------------------------------')\n",
    "\n",
    "        # each subject gets EXAMPLES_PER_SUBJECT number of examples (182 to be exact, \n",
    "        # so this section just writes out the the data once there is a full minibatch \n",
    "        # complete.\n",
    "        if ((len(threat_zone_examples) % (BATCH_SIZE * EXAMPLES_PER_SUBJECT)) == 0):\n",
    "            for tz_num, tz in enumerate(tsa.zone_slice_list):\n",
    "\n",
    "                tz_examples_to_save = []\n",
    "\n",
    "                # write out the batch and reset\n",
    "                print(' -> writing: ' + PREPROCESSED_DATA_FOLDER + \n",
    "                                        'preprocessed_TSA_scans-tz{}-{}-{}-b{}.npy'.format( \n",
    "                                        tz_num+1,\n",
    "                                        len(threat_zone_examples[0][1][0]),\n",
    "                                        len(threat_zone_examples[0][1][1]), \n",
    "                                        batch_num))\n",
    "\n",
    "                # get this tz's examples\n",
    "                tz_examples = [example for example in threat_zone_examples if example[0] == \n",
    "                               [tz_num]]\n",
    "\n",
    "                # drop unused columns\n",
    "                tz_examples_to_save.append([[features_label[1], features_label[2]] \n",
    "                                            for features_label in tz_examples])\n",
    "\n",
    "                # save batch.  Note that the trainer looks for tz{} where {} is a \n",
    "                # tz_num 1 based in the minibatch file to select which batches to \n",
    "                # use for training a given threat zone\n",
    "                np.save(PREPROCESSED_DATA_FOLDER + \n",
    "                        'preprocessed_TSA_scans-tz{}-{}-{}-b{}.npy'.format(tz_num+1, \n",
    "                                                         len(threat_zone_examples[0][1][0]),\n",
    "                                                         len(threat_zone_examples[0][1][1]), \n",
    "                                                         batch_num), \n",
    "                                                         tz_examples_to_save)\n",
    "                del tz_examples_to_save\n",
    "\n",
    "            #reset for next batch \n",
    "            del threat_zone_examples\n",
    "            threat_zone_examples = []\n",
    "            batch_num += 1\n",
    "    \n",
    "    # we may run out of subjects before we finish a batch, so we write out \n",
    "    # the last batch stub\n",
    "    if (len(threat_zone_examples) > 0):\n",
    "        for tz_num, tz in enumerate(tsa.zone_slice_list):\n",
    "\n",
    "            tz_examples_to_save = []\n",
    "\n",
    "            # write out the batch and reset\n",
    "            print(' -> writing: ' + PREPROCESSED_DATA_FOLDER \n",
    "                    + 'preprocessed_TSA_scans-tz{}-{}-{}-b{}.npy'.format(tz_num+1, \n",
    "                      len(threat_zone_examples[0][1][0]),\n",
    "                      len(threat_zone_examples[0][1][1]), \n",
    "                                                                                                                  batch_num))\n",
    "\n",
    "            # get this tz's examples\n",
    "            tz_examples = [example for example in threat_zone_examples if example[0] == \n",
    "                           [tz_num]]\n",
    "\n",
    "            # drop unused columns\n",
    "            tz_examples_to_save.append([[features_label[1], features_label[2]] \n",
    "                                        for features_label in tz_examples])\n",
    "\n",
    "            #save batch\n",
    "            np.save(PREPROCESSED_DATA_FOLDER + \n",
    "                    'preprocessed_TSA_scans-tz{}-{}-{}-b{}.npy'.format(tz_num+1, \n",
    "                                                     len(threat_zone_examples[0][1][0]),\n",
    "                                                     len(threat_zone_examples[0][1][1]), \n",
    "                                                     batch_num), \n",
    "                                                     tz_examples_to_save)\n",
    "# unit test ---------------------------------------\n",
    "#preprocess_tsa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test ---------------------------------------\n",
    "preprocess_tsa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# get_train_test_file_list(): gets the batch file list, splits between train and test\n",
    "#\n",
    "# parameters:      none\n",
    "#\n",
    "# returns:         none\n",
    "#\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def get_train_test_file_list():\n",
    "    \n",
    "    global FILE_LIST\n",
    "    global TRAIN_SET_FILE_LIST\n",
    "    global TEST_SET_FILE_LIST\n",
    "    \n",
    "    if os.listdir(PREPROCESSED_DATA_FOLDER) == []:\n",
    "        print ('No preprocessed data available.  Skipping preprocessed data setup..')\n",
    "    else:\n",
    "        FILE_LIST = [f for f in os.listdir(PREPROCESSED_DATA_FOLDER)] \n",
    "#                      if re.search(re.compile('-tz' + str(THREAT_ZONE) + '-'), f)]\n",
    "        train_test_split = len(FILE_LIST) - \\\n",
    "                           max(int(len(FILE_LIST)*TRAIN_TEST_SPLIT_RATIO),1)\n",
    "        TRAIN_SET_FILE_LIST = FILE_LIST[:train_test_split]\n",
    "        TEST_SET_FILE_LIST = FILE_LIST[train_test_split:]\n",
    "        print('Train/Test Split -> {} file(s) of {} used for testing'.format( \n",
    "              len(FILE_LIST) - train_test_split, len(FILE_LIST)))\n",
    "        \n",
    "# unit test ----------------------------\n",
    "#get_train_test_file_list()\n",
    "#print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test ----------------------------\n",
    "get_train_test_file_list()\n",
    "print('Train set: \\n', TRAIN_SET_FILE_LIST, '\\n')\n",
    "print('Test set: \\n', TEST_SET_FILE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# input_pipeline(filename, path): prepares a batch of features and labels for training\n",
    "#\n",
    "# parameters:      filename - the file to be batched into the model\n",
    "#                  path - the folder where filename resides\n",
    "#\n",
    "# returns:         feature_batch - a batch of features to train or test on\n",
    "#                  label_batch - a batch of labels related to the feature_batch\n",
    "#\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def input_pipeline(filename, path):\n",
    "\n",
    "    preprocessed_tz_scans = []\n",
    "    feature_batch = []\n",
    "    label_batch = []\n",
    "    \n",
    "    #Load a batch of preprocessed tz scans\n",
    "    preprocessed_tz_scans = np.load(os.path.join(path, filename))\n",
    "        \n",
    "    #Shuffle to randomize for input into the model\n",
    "    np.random.shuffle(preprocessed_tz_scans)\n",
    "    \n",
    "    # separate features and labels\n",
    "#     print('preprocess scans :', preprocessed_tz_scans)\n",
    "    for example_list in preprocessed_tz_scans:\n",
    "#         print('preprocess scans :', example_list)\n",
    "        for example in example_list:\n",
    "#             print('scans :', example)\n",
    "            feature_batch.append(example[0])\n",
    "            label_batch.append(example[1])\n",
    "    \n",
    "    feature_batch = np.asarray(feature_batch, dtype=np.float32)\n",
    "    label_batch = np.asarray(label_batch, dtype=np.float32)\n",
    "    \n",
    "    return feature_batch, label_batch\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test ------------------------------------------------------------------------\n",
    "print ('Train Set -----------------------------')\n",
    "for f_in in TRAIN_SET_FILE_LIST:\n",
    "   feature_batch, label_batch = input_pipeline(f_in, PREPROCESSED_DATA_FOLDER)\n",
    "   print (' -> features shape {}:{}:{}'.format(len(feature_batch), \n",
    "                                               len(feature_batch[0]), \n",
    "                                               len(feature_batch[0][0])))\n",
    "   print (' -> labels shape   {}:{}'.format(len(label_batch), len(label_batch[0])))\n",
    "    \n",
    "print ('Test Set -----------------------------')\n",
    "for f_in in TEST_SET_FILE_LIST:\n",
    "   feature_batch, label_batch = input_pipeline(f_in, PREPROCESSED_DATA_FOLDER)\n",
    "   print (' -> features shape {}:{}:{}'.format(len(feature_batch), \n",
    "                                               len(feature_batch[0]), \n",
    "                                               len(feature_batch[0][0])))\n",
    "   print (' -> labels shape   {}:{}'.format(len(label_batch), len(label_batch[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# shuffle_train_set(): shuffle the list of batch files so that each train step\n",
    "#                      receives them in a different order since the TRAIN_SET_FILE_LIST\n",
    "#                      is a global\n",
    "#\n",
    "# parameters:      train_set - the file listing to be shuffled\n",
    "#\n",
    "# returns:         none\n",
    "#\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def shuffle_train_set(train_set):\n",
    "    sorted_file_list = random.shuffle(train_set)\n",
    "    TRAIN_SET_FILE_LIST = sorted_file_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test ---------------\n",
    "print ('Before Shuffling ->', TRAIN_SET_FILE_LIST)\n",
    "shuffle_train_set(TRAIN_SET_FILE_LIST)\n",
    "print ('After Shuffling ->', TRAIN_SET_FILE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# alexnet(width, height, lr): defines the alexnet\n",
    "#\n",
    "# parameters:      width - width of the input image\n",
    "#                  height - height of the input image\n",
    "#                  lr - learning rate\n",
    "#\n",
    "# returns:         none\n",
    "#\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def alexnet(width, height, lr):\n",
    "    network = input_data(shape=[None, width, height, 1], name='features')\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum', loss='categorical_crossentropy', \n",
    "                         learning_rate=lr, name='labels')\n",
    "\n",
    "    model = tflearn.DNN(network, checkpoint_path=MODEL_PATH + MODEL_NAME, \n",
    "                        tensorboard_dir=TRAIN_PATH, tensorboard_verbose=3, max_checkpoints=1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# train_conv_net(): runs the train op\n",
    "#\n",
    "# parameters:      none\n",
    "#\n",
    "# returns:         none\n",
    "#\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def train_conv_net():\n",
    "    \n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "    \n",
    "    # get train and test batches\n",
    "    get_train_test_file_list()\n",
    "    \n",
    "    # instantiate model\n",
    "    model = alexnet(IMAGE_DIM, IMAGE_DIM, LEARNING_RATE)\n",
    "    \n",
    "    # read in the validation test set\n",
    "    for j, test_f_in in enumerate(TEST_SET_FILE_LIST):\n",
    "        if j == 0:\n",
    "            val_features, val_labels = input_pipeline(test_f_in, PREPROCESSED_DATA_FOLDER)\n",
    "        else:\n",
    "            tmp_feature_batch, tmp_label_batch = input_pipeline(test_f_in, \n",
    "                                                                PREPROCESSED_DATA_FOLDER)\n",
    "            val_features = np.concatenate((tmp_feature_batch, val_features), axis=0)\n",
    "            val_labels = np.concatenate((tmp_label_batch, val_labels), axis=0)\n",
    "\n",
    "    val_features = val_features.reshape(-1, IMAGE_DIM, IMAGE_DIM, 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    # start training process\n",
    "    for i in range(N_TRAIN_STEPS):\n",
    "\n",
    "        # shuffle the train set files before each step\n",
    "        shuffle_train_set(TRAIN_SET_FILE_LIST)\n",
    "        \n",
    "        # run through every batch in the training set\n",
    "        for f_in in TRAIN_SET_FILE_LIST:\n",
    "            \n",
    "            # read in a batch of features and labels for training\n",
    "            feature_batch, label_batch = input_pipeline(f_in, PREPROCESSED_DATA_FOLDER)\n",
    "            feature_batch = feature_batch.reshape(-1, IMAGE_DIM, IMAGE_DIM, 1)\n",
    "            #print ('Feature Batch Shape ->', feature_batch.shape)                \n",
    "                \n",
    "            # run the fit operation\n",
    "            model.fit({'features': feature_batch}, {'labels': label_batch}, n_epoch=1, \n",
    "                      validation_set=({'features': val_features}, {'labels': val_labels}), \n",
    "                      shuffle=True, snapshot_step=None, show_metric=True, \n",
    "                      run_id=MODEL_NAME)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test -----------------------------------\n",
    "train_conv_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
